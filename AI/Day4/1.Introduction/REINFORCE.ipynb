{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy-based method: REINFORCE\n",
    "\n",
    "Now that we've implemented a value-based algorithm, it's only right that we should try out a policy-based one as well, right ? So let's learn about REINFORCE !\n",
    "\n",
    "**Key facts**:\n",
    "- It was first defined in ['Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning' by Ronald J. WILLIAMS in 1992](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf?pdf=button).\n",
    "- It uses a **monte carlo** method\n",
    "\n",
    "### Monte Carlo\n",
    "\n",
    "As explained in the previous notebook, you can think of Monte Carlo as a method in which our agent learns after each episode instead of doing so at each time step like Temporal Difference.\n",
    "\n",
    "This implies that there is no need to estimate the target: we can compute the episodic reward for each timestep using the memory batch:\n",
    "\n",
    "![Monte Carlo formula](./assets/fig13.svg)\n",
    "\n",
    "Let's begin by implementing this formula !\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we begin, let's import some modules and define some constants...\\\n",
    "Notice this time we're using pytorch because it will make it easier for us to deal with optimization since pytorch has a built in 'Adam' optimizer which will improve our `REINFORCE` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Set the learning rate and discount factor\n",
    "lr = 1e-3\n",
    "gamma = 0.995\n",
    "\n",
    "# Set the number of episodes to run\n",
    "episodes = 300"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "From now on, we will be using a popular RL framework called OpenAI Gym !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# Set the environment to use\n",
    "env_name = 'CartPole-v1'\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will `make` our gym environment by providing an environment name. Here, we choose the [Cartpole environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). Feel free to take some time to read its documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cart pole](./assets/fig14.gif)\n",
    "> gif representing the cartpole environment taken from the official documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what information we can retrieve from the `env` variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Action space: {env.action_space} ({env.action_space.n} possible actions)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is `Discrete(2)`, a discrete action space with 2 possible actions.\n",
    "\n",
    "A discrete action space means there is a finite set of actions that the agent can take, for example going left or right. On the contrary, a continuous action space means the actions can depend on various variables, like for example all the different ways you can move a pawn, as well as all the different ways you can move a knight and so on in a chess game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Observation space: {env.observation_space}')\n",
    "print()\n",
    "print(f'State shape: {env.observation_space.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Observation space is of shape (4,) meaning it has 4 values. We've printed the maximum and minimum for each of these values above.\n",
    "\n",
    "You can see that the first value's minimum is `-4.8` and it's maximum is `4.8`.\\\n",
    "It corresponds to the cart's position.\n",
    "\n",
    "The second and fourth values ranges from `-infinity` to `infinity` (`3.8e+38` representing infinity in this case).\\\n",
    "These values correspond to the Cart's velocity and the Pole's angular velocity respectively.\n",
    "\n",
    "The third value ranges from `-0.42` to `0.42`.\\\n",
    "It represents the pole's angle in radians.\n",
    "\n",
    "[Read this part of the documentation for more details](https://www.gymlibrary.dev/environments/classic_control/cart_pole/#observation-space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, know that we have these values, we have all we need to build our neural network because we know what our input size and action size are !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our Neural Network which we will use as our policy function.\\\n",
    "Its input will be the environment's state and its output will be a list of probabilities for each action.\\\n",
    "You can do whatever you want with the hidden layer(s).\n",
    "\n",
    "As for the activations, apply ReLU for the first linear function followed by softmax for the output layer.\n",
    "\n",
    "> Use `env` to access the input and output sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network to model the policy\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create fully-connected layers with ReLU activations\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "\n",
    "        self.actions, self.states, self.rewards = [], [], []\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert the input tensor to a float tensor\n",
    "\n",
    "        # Apply ReLU activations to the fully-connected layers\n",
    "\n",
    "        # Apply a softmax activation to the final layer, to get probabilities for each action\n",
    "\n",
    "        return x\n",
    "\n",
    "network = NeuralNetwork(env)\n",
    "\n",
    "# Use Adam optimizer to optimize the neural network\n",
    "optim = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](./assets/fig16.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now let's see what else gym can do !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "\n",
    "print(state)\n",
    "print()\n",
    "print(info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `env.reset()`, we have access to the four values inside our state ! If you reload this cell, you'll notice that these values are initialized randomly.\n",
    "\n",
    "We also receive an empty dictionary which for other environments can contain additional information.\\\n",
    "From now on, we will be receiving `state, _` from `env.reset()` because we don't have any need for the `info` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample() # use this method to get a random action from the action space\n",
    "print(f\"We choose the action {action}...\")\n",
    "\n",
    "new_state, reward, termination, truncation, _ = env.step(action) # the last return is the info dictionary\n",
    "\n",
    "print(\"And we receive:\")\n",
    "print()\n",
    "print(f'Our new state: {new_state}') \n",
    "print(f'The reward: {reward}')\n",
    "print(f'Whether our episode was terminated: {termination}')\n",
    "print(f'Or truncated: {truncation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env.render())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env.render()` returns an rgb array representing our environment which we can plot using matplotlib's `imshow()` method !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "for _ in range(5):\n",
    "    env.reset()\n",
    "    termination = False\n",
    "    while termination is not True:\n",
    "        _, _, termination, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple loop, we can see how our agent fares when it chooses an action at random. Not so well, huh ? Well let's train it using REINFORCE and see how it improves !\n",
    "\n",
    "Here's the REINFORCE algorithm as defined in Chapter 13 of [Sutton and Bartol's 'Reinforcement Learning: an Introduction'](http://incompleteideas.net/book/RLbook2020.pdf):\n",
    "\n",
    "![REINFORCE algorithm](./assets/fig15.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by setting up a few lists we'll be using for logging our reward and loss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the deque class from the collections module\n",
    "from collections import deque\n",
    "\n",
    "# Initialize empty lists for rewards and losses\n",
    "recent_rewards = deque(maxlen=100)\n",
    "train_rewards = []\n",
    "train_loss = []\n",
    "\n",
    "# We will avoid rendering our environment during training: \n",
    "# it would tremendously slow down the process\n",
    "env = gym.make(env_name) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's up to you to implement the REINFORCE algorithm using OpenAI Gym's Cartpole environment !\n",
    "\n",
    "Let's begin by defining our policy:\n",
    "\n",
    ">- Create a `policy_action()` method which returns an action based on the policy\n",
    ">- Check out [Pytorch's Categorial Class](https://pytorch.org/docs/stable/distributions.html) which provides a great tool for probability distributions.\\\n",
    ">The provided link explains its usage within REINFORCE in particular ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def policy_action(self, state):\n",
    "    # Get the probabilities for each action, using the current state\n",
    "\n",
    "    # Create a distribution according to the probabilities\n",
    "\n",
    "    # Sample an action from the distribution\n",
    "\n",
    "    # Return the chosen action\n",
    "    pass\n",
    "\n",
    "NeuralNetwork.policy_action = policy_action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we need to define a simple method which stores our `action, state, reward` tuple at each time step.\n",
    "\n",
    ">- Simply add the arguments, `Action, State, Reward`, to their respective lists inside the `network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(self, Action, State, Reward):\n",
    "    pass\n",
    "    \n",
    "NeuralNetwork.remember = remember"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to compute our discounted rewards for each time step, in other words, the 'G' variable in the algorithm:\n",
    "\n",
    "You can think of discounting as a way to help the agent become a better long-term planner as opposed to a short-term opportunist. We do this by discounting the value of rewards based on the time step.\n",
    "\n",
    "$$ G = \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_k $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means:\n",
    "\n",
    "- We need to return a list of `T` numbers (`T` being the episode length in steps)\n",
    "  - Each of these numbers ( $ \\sum^{T}_{k=t+1} $ ) is defined as such: \n",
    "    - $ \\gamma^{k-t-1} * R_k $\n",
    "\n",
    "- You are free to achieve this using either loops or `numpy` methods like [`power()`](https://numpy.org/doc/stable/reference/generated/numpy.power.html) and [`cumsum`](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html)\n",
    "\n",
    "- Don't forget we've declared a `gamma` constant at the top of the notebook !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(self):\n",
    "    ## Discount the returns using the discount factor\n",
    "    pass\n",
    "\n",
    "NeuralNetwork.discount_rewards = discount_rewards\n",
    "\n",
    "network.rewards = [0.2, 0.6, 0.1, 1.2, 0.9]\n",
    "network.discount_rewards()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: `[2.9602269 , 2.7602269 , 2.1632269 , 2.0642244 , 0.88213455]`\n",
    "> If your values are close to these, it means you've correctly implemented the discounting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's implement our gradient ascent !\n",
    "\n",
    "Since Adam, our optimizer, does most of the job for us, we only need to provide it the correct loss.\\\n",
    "In REINFORCE, the loss is defined as follows:\n",
    "\n",
    "$$ L = G \\nabla \\ln \\pi(A_t|S_t,\\theta) $$\n",
    "\n",
    "The $ \\nabla $ symbol represents the gradient, so you can understand this formula as: `loss = G * log_prob` \n",
    "\n",
    "Because we are attempting to find the parameters of our policy which **maximize** the expected cumulative reward, we will not use the familiar **gradient descent** and instead use what is known as **gradient ascent**.\n",
    "\n",
    "Fear not, because it is not very complicated !\n",
    "\n",
    "With gradient descent, we update the parameters in the opposite direction of the gradient, which decreases the training cost or the expected cumulative reward, in this case.\n",
    "\n",
    "So in order to use gradient **ascent**, we must aim to **increase** the expected cumulative reward. Which means we \n",
    "can achieve this by simply doing a `backward()` on a negative loss !\n",
    "\n",
    "This leads us back to the Monte Carlo formula for value-based methods:\n",
    "\n",
    "$$ G_t - V(S_t) $$\n",
    "\n",
    "or, for policy-based methods:\n",
    "\n",
    "$$ G[- \\nabla \\ln \\pi(A_t|S_t,\\theta)] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(self, discounted_rewards):\n",
    "    # Perform gradient ascent to update the probabilities in the distribution\n",
    "    for State, Action, G in zip(self.states, self.actions, discounted_rewards):\n",
    "        # Get the probabilities for the current state\n",
    "        probs = None\n",
    "\n",
    "        # Calculate the loss as the negative log probability of the chosen action\n",
    "        # multiplied by the discounted return\n",
    "        loss = None\n",
    "\n",
    "        # Clear the gradients, backpropagate the loss, and update the network parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        #\n",
    "\n",
    "NeuralNetwork.gradient_ascent = gradient_ascent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to use this `NeuralNetwork` class inside a training loop to make our agent learn how to solve Cartpole using the REINFORCE algorithm !\n",
    "\n",
    "We'll leave this part up to you ! (Use the screenshot of the REINFORCE pseudo code from earlier in the notebook for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the number of episodes\n",
    "for episode in range(episodes):\n",
    "    # Reset the environment and initialize empty lists for actions, states, and rewards\n",
    "    state, _  = env.reset()\n",
    "    network.actions, network.states, network.rewards = [], [], []\n",
    "\n",
    "    # Train the agent for a single episode\n",
    "    for _ in range(1000):\n",
    "        action = network.policy_action(state)\n",
    "\n",
    "        # Take the action in the environment and get the new state, reward, and done flag\n",
    "        new_state, reward, termination, truncation, _ = env.step(action)\n",
    "\n",
    "        # Save the action, state, and reward for later\n",
    "        network.remember(action, state, reward)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # If the episode is done or the time limit is reached, stop training\n",
    "        if termination or truncation:\n",
    "            break\n",
    "\n",
    "    # Perform gradient ascent\n",
    "    network.gradient_ascent(network.discount_rewards())\n",
    "\n",
    "    # Save the total reward for the episode and append it to the recent rewards queue\n",
    "    train_rewards.append(np.sum(network.rewards))\n",
    "    recent_rewards.append(train_rewards[-1])\n",
    "\n",
    "    # Print the mean recent reward every 50 episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode {episode:>6}: \\tR:{np.mean(recent_rewards):>6.3f}\")\n",
    "\n",
    "    if np.mean(recent_rewards) > 400:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_rewards)\n",
    "ax.plot(gaussian_filter1d(train_rewards, sigma=20), linewidth=4)\n",
    "ax.set_title('Rewards')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display five episodes of our trained agent to see how glorious it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "for _ in range(5):\n",
    "    Rewards = []\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        # Calculate the probabilities of taking each action using the trained\n",
    "        # neural network\n",
    "        probs = network.forward(state)\n",
    "        \n",
    "        # Sample an action from the resulting distribution using the \n",
    "        # torch.distributions.Categorical() method\n",
    "        action = None\n",
    "    \n",
    "        new_state, reward, termination, truncation, _ = env.step(action)\n",
    "    \n",
    "        state = new_state\n",
    "\n",
    "        Rewards.append(reward)\n",
    "\n",
    "        if termination or truncation:\n",
    "            break\n",
    "    \n",
    "    # Print the total rewards for the current episode\n",
    "    print(f'Reward: {sum(Rewards)}')\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, you really did it ! You've succesfully implemented both a value-based and a policy-based method in reinforcement learning ! And, to top it all off, you even managed to solve CartPole using OpenAI Gym !\n",
    "\n",
    "If you're up for it, let's head over to section 2 and go **deeper** within the field of RL by returning to value-based methods and implementing the successor to Q-Learning, Deep Q Network, or DQN for short !\n",
    "\n",
    "Good luck !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b483bbea0ef867292651300ca303e9b91f9a0c7db919f54df8d16a1790f2d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
