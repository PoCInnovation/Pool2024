{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value-based method: Q Learning\n",
    "\n",
    "In the first notebook of the day, we will learn about one of the most popular RL algorithms: Q Learning !\n",
    "\n",
    "**Key facts**:\n",
    "- [It was first defined in 1989 by Christopher J.C.H. WATKINS](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf?pdf=button)\n",
    "- It uses a **temporal difference (TD)** approach\n",
    "- It is an **Action Value** function\n",
    "- It is **off-policy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Temporal Difference (TD)\n",
    "\n",
    "There are two different learning strategies on how to train a value or policy function :\\\n",
    "One of them is 'Monte Carlo', in which the agent experiences an entire episode of the environment before learning (updating its value function). This means that it stores the state, action and reward inside a memory which it unwraps at the end of each episode.\n",
    "> Monte Carlo will be explained in more detail inside the `REINFORCE.ipynb` notebook.\n",
    "\n",
    "Q-Learning uses 'Temporal Difference', which means it learns at each time step of the environment. In other words, the agent updates its value function using the current state, action, reward and resulting state.\n",
    "\n",
    "![Temporal difference](./assets/fig9.svg)\n",
    "> Formula for temporal difference\n",
    "\n",
    "Don't let the mathematical expressions scare you, all you need to understand is that we update the state's value at each time step by adding the difference between the target and the old value, multiplied by a learning rate, to our old value.\n",
    "\n",
    "If it helps, here is a version of this formula in pseudo code:\n",
    "\n",
    "```py\n",
    "LR = 0.05\n",
    "GAMMA = 0.99\n",
    "\n",
    "state_values = [...] # the list of values for each of our states\n",
    "action = agent_choice(state) # choosing an action based on the state\n",
    "new_state, reward = environment_step(action) # retrieving a new state and a reward from the environment\n",
    "\n",
    "target = reward + GAMMA * state_values[new_state] # computing the target\n",
    "state_values[state] = state_values[state] + LR * (target - state_values[state]) # updating the state value \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Action value\n",
    "\n",
    "Q-Learning is a value-based function and there are also two different types of those as well !\n",
    "> We promise, these ones are easy to differentiate !\n",
    "\n",
    "- State-value functions, where each state has a different value\n",
    "- Action-value functions, where each (state,action) pair has a different value\n",
    "\n",
    "![Action and state values](./assets/fig10.svg)\n",
    "\n",
    "Notice how there are (state,action) pairs where the value is 0. That is because our agent never performed the actions at those states."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perhaps it's time to take a short break from the theory and get into implementation, shall we ?\n",
    "> You'll see, it'll be much easier to understand if you take it all one step at a time !\n",
    "\n",
    "Let's begin by importing some libraries and defining some constants..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from seaborn import heatmap\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from IPython.display import Image\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Import the Environment class from the envi module\n",
    "from envi import Environment\n",
    "\n",
    "# Define the actions that the agent can take\n",
    "ACTIONS = {'UP': 0, 'LEFT': 1, 'DOWN': 2, 'RIGHT': 3}\n",
    "\n",
    "# Define the size of the gridworld\n",
    "MAP_SIZE = 10\n",
    "\n",
    "# Define the number of episodes to train for\n",
    "EPISODES = 10_000\n",
    "\n",
    "# Define the learning rate\n",
    "LR = 5e-3\n",
    "\n",
    "# Define the discount factor\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an `Agent` class which will contain our action values and a method to update them !\n",
    "\n",
    "In Q-Learning, the table which contains our action values is called the Q-Table ! (catchy, right ?)\n",
    "\n",
    "In order to define this Q-Table, we simply create an array of shape `(number_of_states, number_of_actions)` and initialize all of its values to 0.\n",
    "You can use `numpy`'s `zeros()` method to achieve this by defining the `self.q_table` property inside the `__init__()` method of our Agent.\n",
    "\n",
    "Our environment is a grid world, so you can consider each square a separate state, meaning that if our grid is of size 2 * 2, the number of states is 4.\n",
    "\n",
    "\n",
    "| Action     | State 1 | State 2 | State 3 | State 4 |\n",
    "| ---------- | ------- | ------- | ------- | ------- |\n",
    "| **UP**     | 0.0     | 0.0     | 0.0     | 0.0     |\n",
    "| **DOWN**   | 0.0     | 0.0     | 0.0     | 0.0     |\n",
    "| **RIGHT**  | 0.0     | 0.0     | 0.0     | 0.0     |\n",
    "| **LEFT**   | 0.0     | 0.0     | 0.0     | 0.0     |\n",
    "> This is what a fresh Q-Table of a 4 * 4 gridworld environment with 4 actions should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    This class defines our Agent which will interact with the environment and update its Q Table\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We initialize a value called `epsilon` (we will soon learn more about it)\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        # Initialize the Q Table for the agent with zeros\n",
    "        self.q_table = None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that was easy. Let's implement the Q-Function now !\\\n",
    "Remember the TD formula ? Well, the Q-Function is the same as that one, except we are updating the action-value, not the state-value:\n",
    "\n",
    "Knowing this, let's update our temporal difference formula using Action values instead:\n",
    "\n",
    "![Q-Learning formula](./assets/fig11.svg)\n",
    "\n",
    "Now, define a new method which implements this formula in python code !\n",
    "\n",
    "> - The `update_q_table()` method doesn't need to return anything, you must update the q_table directly inside the method.\n",
    "> - Refer to the `Agent` class above if you don't remember the contents of `self` and how they can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(self, new_state, state, action, reward):\n",
    "    \"\"\"\n",
    "    This method updates the Q Table\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "Agent.update_q_table = update_q_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Off-policy\n",
    "\n",
    "There is one more useful concept you need to understand before we continue !\n",
    "\n",
    "Because Q-Learning is off-policy, we do not know which action to choose for any given state.\\\n",
    "All we have is our Q-Table, which contains valuable information which our agent will need in order to form an opinion.\n",
    "\n",
    "But how should the agent form an opinion ?\n",
    "\n",
    "An easy answer is to simply pick the action with the highest value.\\\n",
    "This is called a **greedy policy**.\n",
    "\n",
    "There is one flaw with this policy, though. While it is a good idea to pick the action that we believe is optimal, we do not have access to an optimal policy. Therefore, while a greedy policy works once our agent is well trained (because that means our estimated policy will be close to the optimal policy), it will not work quite as well if the agent is barely discovering the environment.\n",
    "\n",
    "Imagine this scenario:\n",
    "\n",
    "![Greedy policy flaw](./assets/fig12.svg)\n",
    "\n",
    "Our agent has two choices: either left or right ! \n",
    "\n",
    "If he chooses left, he receives +10 reward !!!\\\n",
    "On the other hand, if he chooses right, he receives a measly +1 reward...\n",
    "\n",
    "Alas, robot boy goes to the right on his first attempt, while both action-values are 0.\\\n",
    "Because of this, he believes going right is the best choice, despite never having attempted to go left !\n",
    "\n",
    "It is a bit like deciding you hate sitcoms because you've only ever seen 'Big Bang Theory' and you hated it.\\\n",
    "But because of your **greedy policy**, you miss out on a show like 'Seinfeld' ! What a bummer !\n",
    "\n",
    "Thankfully, there's another policy you can try: **Epsilon-Greedy policy** !\n",
    "\n",
    "With the epsilon greedy policy, you start by picking actions at random before gradually choosing the actions you value !\n",
    "\n",
    "```py\n",
    "epsilon = 1.0\n",
    "\n",
    "for i in range(1000):\n",
    "    use_greedy_policy = random.random() > epsilon # use greedy policy with a probability of epsilon\n",
    "    if use_greedy_policy: # if epsilon is high, this will happen more often\n",
    "        action = greedy_action(state)\n",
    "    else: # if epsilon is low, this will happen more often instead\n",
    "        action = random_action()\n",
    "\n",
    "    epsilon = max(epsilon * 0.995, 0.05) # decaying epsilon so that we gain confidence in our Q-Table (we tend to keep a small probability of random policy during training so we don't go below 0.05)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(self, state):\n",
    "    \"\"\"\n",
    "    This method is an implementation of the epsilon greedy policy\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "Agent.epsilon_greedy_policy = epsilon_greedy_policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm\n",
    "\n",
    "Well, with that out of the way, we've defined a nice Agent class. It will come in handy for the next part, which is training the agent to solve our gridworld environment !\n",
    "\n",
    "Let's start by initializing our Agent and Environment instances, as well as some lists we will use to store our rewards throughout the training for plotting purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and an agent\n",
    "from collections import deque\n",
    "\n",
    "env = Environment(MAP_SIZE, ACTIONS)\n",
    "agent = Agent()\n",
    "\n",
    "# Initialize empty lists for rewards and losses\n",
    "recent_rewards = deque(maxlen=1_000)\n",
    "train_rewards = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to implement the following algorithm (figure taken from [Sutton and Bartol's 'Reinforcement Learning: an Introduction'](http://incompleteideas.net/book/RLbook2020.pdf))\n",
    "\n",
    "![Q-Learning algorithm](./assets/fig8.svg)\n",
    "\n",
    "You've already implemented most of the algorithm inside the `Agent` class. Try to understand which lines correspond to which methods.\n",
    "\n",
    "The initialization is covered by `agent = Agent()` which we declared above.\n",
    "The epsilon greedy policy is a method inside `Agent` and so is the penultimate line of second loop: updating the q-table !\n",
    "\n",
    "The Environment class has two methods you should know about:\n",
    "\n",
    "- `env.reset()` resets the environment and returns a state\n",
    "- `env.step()` updates the environment by taking an action as argument and returns a tuple containing `new_state, reward, done`. The latter is a boolean which tells us whether the episode is terminated or not.\n",
    "\n",
    "With this info, see if you can fill in the blanks and build your Q-Learning algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the number of episodes\n",
    "for episode in range(EPISODES):\n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()\n",
    "\n",
    "    # Initialize empty lists for rewards and losses in this episode\n",
    "    episode_reward = []\n",
    "\n",
    "    # Iterate over the time steps in the episode\n",
    "    for i in range(1000):\n",
    "        action = agent.epsilon_greedy_policy(state)\n",
    "\n",
    "        # Interact with the environment to get the new state, reward, and done flag      \n",
    "\n",
    "        # Set the new state as the current state\n",
    "\n",
    "        # If the episode is done, break out of the loop\n",
    "    \n",
    "    # Log the rewards and losses for this episode\n",
    "    train_rewards.append(np.sum(episode_reward))\n",
    "    recent_rewards.append(train_rewards[-1])\n",
    "\n",
    "    # Print a table of information about the episode every 5,000 episodes\n",
    "    if episode % 1_000 == 0:\n",
    "        print(f\"Episode {episode:>6}: \\tR:{np.mean(recent_rewards):>6.3f}\\t Epsilon:{agent.epsilon:>6.3f}\\t State:{state:>6}\")\n",
    "\n",
    "# Reset the environment to get the initial state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plotting rewards\n",
    "ax.plot(gaussian_filter1d(train_rewards, sigma=10))\n",
    "ax.set_title('Rewards')\n",
    "# show figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, your rewards should increase before reaching a treshold.\n",
    "> This graph depends on the parameters you set at the beginning of the notebook.\\\n",
    "> You can try changing the MAP_SIZE for example for very different results.\\\n",
    "> It is advised to stay below 30 for the MAP_SIZE, otherwise your agent might find that it is a better idea to kill itself rather than reach its goal ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our Q-table and observe our estimated policy as well as the values for each action-state pair !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the optimal actions from the Q-table\n",
    "best_actions = [np.argmax(x) if np.mean(x) != x[0] else -1 for x in agent.q_table]\n",
    "\n",
    "# Initialize a matrix for the policy\n",
    "policy = np.zeros((MAP_SIZE ** 2, len(ACTIONS)))\n",
    "\n",
    "# Fill in the policy matrix\n",
    "for y in range(MAP_SIZE ** 2):\n",
    "    for x in range(MAP_SIZE):\n",
    "        if x == best_actions[y]:\n",
    "            policy[y][x] = 1\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "# Plot the policy matrix as a heatmap\n",
    "heatmap(policy, ax=ax[0], xticklabels=ACTIONS, cbar=False)\n",
    "\n",
    "# Plot the Q-table as a heatmap\n",
    "heatmap(agent.q_table, ax=ax[1], xticklabels=ACTIONS, annot=MAP_SIZE<6)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code extracts the optimal actions from the Q-table and uses them to create a matrix representing the policy. It then plots the policy matrix and the Q-table as heatmaps. The policy matrix shows which actions are considered optimal in which states, while the Q-table shows the values of the actions in each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's record a video of our agent solving the grid world !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# Iterate over the time steps in the episode\n",
    "for i in range(1000):\n",
    "    # Add the current state of the environment to the list of frames\n",
    "    frames.append(np.array(env.graphic()))\n",
    "\n",
    "    # Choose the greedy action for the current state\n",
    "    action = agent.epsilon_greedy_policy(state)\n",
    "\n",
    "    # Interact with the environment to get the new state, reward, and done flag\n",
    "    new_state, reward, done = env.step(action)\n",
    "\n",
    "    # Set the new state as the current state\n",
    "    state = new_state\n",
    "\n",
    "    # If the episode is done, reset the environment and break out of the loop\n",
    "    if done is True:\n",
    "        frames.append(np.array(env.graphic()))\n",
    "        state = env.reset()\n",
    "        break\n",
    "\n",
    "clip = ImageSequenceClip(list(frames), fps=20)\n",
    "clip.resize(width=300)\n",
    "clip.write_gif('output.gif', fps=20)\n",
    "Image('output.gif', width=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Awesome ! You've managed to implement the Q-Learning algorithm in python !\n",
    "\n",
    "Isn't it cool to see the agent go from not knowing anything to seamlessly running through the maze while avoiding all obstacles ?\n",
    "\n",
    "What happens if the maze is bigger, though ? Increase the value of the `MAP_SIZE` constant at the top of the notebook to see the changes. Beware though; this environment is not suited for large sizes, so stay below 30 if you want good results. Also, the larger the size, the long it will take your agent to learn. You can also change the episode count and length, if you want !\n",
    "\n",
    "Hopefully this notebook was fun. We decided to spare you the creation of the environment because that doesn't teach you anything about AI and it would be a little time consuming for what it's worth.\n",
    "\n",
    "In the next notebook, `REINFORCE.ipynb`, we'll show you a great tool that is used in RL to easily deal with pre-made environments which are tailor-made for RL ! We will also be implementing a policy-based, on-policy, monte carlo algorithm !\\\n",
    "Basically the opposite to Q-Learning !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pool')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b483bbea0ef867292651300ca303e9b91f9a0c7db919f54df8d16a1790f2d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
