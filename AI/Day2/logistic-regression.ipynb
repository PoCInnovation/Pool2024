{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "EPOCH = 100\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2024 ~\n",
    "- ## Day 2: Neural Networks from Scratch\n",
    "    - ### Module 2: Logistic Regression\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first machine learning algorithm ! You were probably getting really impatient of diving into AI. I hope you understand why we wanted to take the time to go through all the basics first, though, because as you could probably tell, our python and numpy skills are going to prove really useful when building machine learning models.\n",
    "\n",
    "During the first module of the day, you got the gist of the main aspects of a machine learning pipeline:\n",
    "- making a prediction\n",
    "- computing the loss\n",
    "- computing the gradients\n",
    "- updating the weight and bias\n",
    "\n",
    "You'll find that this basic architecture is behind almost everything we'll be doing for the rest of the week.\n",
    "\n",
    "You will also find that this basic architecture could be recycled so that the developer can focus entirely on the things that do change.\n",
    "\n",
    "For example, here's an example of how **Linear Regression** can be achieved using the most popular ML library, **pytorch**:\n",
    "\n",
    "```python\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc = nn.Linear(***,***)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "And actually, here's the same for **Logistic Regression**, which is what we'll be implementing by hand in this module !\n",
    "\n",
    "```python\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc = nn.Linear(***,***)\n",
    "    def forward(self, x):\n",
    "        x = self.fc\n",
    "        return F.sigmoid(x)\n",
    "```\n",
    "\n",
    "Cool right ? Well, it might look great for Linear Regression, since you already know what's going on behind the scenes...\\\n",
    "But unless you already know how Logistic Regression works, the code sample won't tell you anything !\n",
    "\n",
    "That's why we're taking the time to learn the (boring?) math behind these algorithms. It might be annoying at first, but I can assure you that understanding why we use Linear instead of Logistic Regression for certain tasks is much more intuitive if you know how they work than if you have no idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we're going to be using an actual ML library before we begin !\n",
    "\n",
    "The library is called sklearn and it is a wonderful set of tools which can help while working on AI !\n",
    "\n",
    "In fact, sklearn has implementations of many algorithms, including Linear and Logistic Regression !\n",
    "\n",
    "It also provides us with plenty of tools to quickly generate and manipulate randomized data for training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using `make_blobs()`, we generate a sample dataset with 1_000 entries, each with two features.\n",
    "## With the `centers` parameter, we tell sklearn to separate the data in two main classes\n",
    "## Logistic Regression being a classifier model, we will use it to predict if one data entry\n",
    "## belongs to one class or the other !\n",
    "x_train, y_train = sklearn.datasets.make_blobs(n_samples=1_000, n_features=2, centers=2)\n",
    "\n",
    "## This data doesn't mean anything, like Brad's problem in the last module, but if you're wondering\n",
    "## how multiple features would translate into a real world problem, imagine if you had data of\n",
    "## house prices and their size, and you needed to predict whether Brad would be willing to buy the\n",
    "## house or not. That would mean each data entry would have two features: the price and size of the\n",
    "## house. The \"x\" would be an array of [price, size] and \"y\" would be a binary value (either true or false).\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use matplotlib to display our data in a nice way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train[:,0], x_train[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry is represented by a blue circle, and you can clearly see that there are two clearly separate groups of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use sklearn's `SGDClassifier` to train a logistic regression on our generated data.\n",
    "\n",
    "If you're curious, you might stumble upon `LogisticRegression` while browsing through [sklearn's docs](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning), which is also a sklearn model which implements the eponymous algorithm.\n",
    "\n",
    "The reason we use `SGDClassifier` instead is because it adds the notion of gradient descent and updating weights to the basic Logistic Regression algorithm.\n",
    "\n",
    ">SGD stands for 'stochastic gradient descent' btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = SGDClassifier(eta0=LR, max_iter=EPOCH)\n",
    "mdl.fit(x_train, y_train)\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=mdl.predict(x_train))\n",
    "plt.show()\n",
    "print(f\"{(mdl.predict(x_train) == y_train).mean()*100}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, sklearn was able to fit a model to our data pretty quickly and get an accuracy extremely close to 100%.\\\n",
    "We've colored our data so that you can clearly see how every entry is classified by the sklearn model.\\\n",
    "Keep this in mind, because we'll compare it to our own model's predictions later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve the same results, let's build our model to be used the same way as sklearn, so we'll have our own `fit()` and `predict()` methods.\\\n",
    "And, just like sklearn, we'll wrap our model inside a class so that it can keep track of its weights and biases.\n",
    "\n",
    "> Storing all of our model's functionalities inside a class will also allow us to save and load the model's weights and biases easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and create your own `MyLogisticRegression` class :\n",
    "\n",
    "* It should have an `__init__()` method which receives and stores the amount of epochs (`max_iter`) and the learning rate (`lr`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, max_iter=EPOCH, lr=LR):\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "\n",
    "assert MyLogisticRegression().max_iter == EPOCH, \"we can't find max_iter inside your class\"\n",
    "assert MyLogisticRegression().lr == LR, \"we can't find lr inside your class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now, let's build our `fit()` method.\n",
    "\n",
    "> We'd like to start with this method so that you have an overview of which methods must be implemented.\\\n",
    "> So, for now, the method won't work because it will call other methods which are not implemented yet !\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, x: np.ndarray, y: np.ndarray, train=True):\n",
    "    \"\"\"\n",
    "    The fit method initialises the weights and biases, then runs the training loop\n",
    "    in order to fit these weights and biases to the input data.\n",
    "    \"\"\"\n",
    "    ## Initialise the weights and biases to 0\n",
    "    ## Keep in mind that we have only one layer\n",
    "    ## While both of these values must be initialised as 0, they won't be initialised the same way\n",
    "    ## Try to remember the properties and use of both of these values if you can't remember why\n",
    "    self.w = np.zeros(x.shape[1])\n",
    "    self.b = 0\n",
    "\n",
    "    assert np.mean(self.w) == 0, \"w should be initialised to 0\"\n",
    "    assert self.b == 0, \"b should be initialised to 0\"\n",
    "\n",
    "    if train is False:\n",
    "        return\n",
    "\n",
    "    ## We'll call the training loop for you here\n",
    "    ## It is the same as for any other model\n",
    "    ## What changes for logistic regression are the forward and backward methods\n",
    "    for i in range(self.max_iter):\n",
    "        y_pred = self._forward(x)\n",
    "        loss = self._bce(y_pred, y)\n",
    "        dw, db = self._backward(x, y)\n",
    "        self._optimize(dw, db)\n",
    "\n",
    "MyLogisticRegression.fit = fit\n",
    "MyLogisticRegression().fit(x_train, y_train, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the `predict()` method out of the way so that we already have all the visible methods.\n",
    "\n",
    "In a logistic regression model, the outputs, instead of being a prediction for a new value, are a prediction of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification, the model has one output value which is the probability of the input belonging to one class or the other.\n",
    "\n",
    "<img width=\"50%\" src=\"assets/image-9.png\"/>\n",
    "<br>\n",
    "<br>\n",
    "In the picture above, the model's first output is `0.6`. This means that the model's prediction is `1`.\n",
    "<br>\n",
    "<br>\n",
    "<img width=\"50%\" src=\"assets/image-8.png\"/>\n",
    "<br>\n",
    "<br>\n",
    "If, on the other hand, the model's output is closer to 0, like in the second example, the prediction will be `0`.\n",
    "<br>\n",
    "<br>\n",
    "<img width=\"50%\" src=\"assets/image-10.png\"/>\n",
    "<br>\n",
    "<br>\n",
    "For a multi-class classification, the model outputs one probability per class and we take the highest probability as the model's prediction.\n",
    "<br>\n",
    "In this case, the highest probability is stored in the third neuron, so the prediction is `2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about binary classification though, so the `predict()` method must simply output 0 or 1 depending on the input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    The predict method returns its predictions for the given input data.\n",
    "    \"\"\"\n",
    "    y_pred = self._forward(x)\n",
    "    return np.array([1 if p > 0.5 else 0 for p in y_pred])\n",
    "\n",
    "MyLogisticRegression.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two more methods which you should already be familiar with.\n",
    "\n",
    "Indeed, the `_optimize()` and `_linear()` methods are the same as in Linear Regression.\n",
    "\n",
    "Go ahead and adapt them to our LogisticRegression class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(self, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    The linear transformation applies the layer's weights and biases to the input data.\n",
    "    \"\"\"\n",
    "    return self.w @ x.T + self.b\n",
    "\n",
    "MyLogisticRegression._linear = linear\n",
    "\n",
    "def optimize(self, dw: np.ndarray, db: np.ndarray):\n",
    "    \"\"\"\n",
    "    The optimize method performs the gradient descent update to the weights and biases.\n",
    "    \"\"\"\n",
    "    self.w -= dw * self.lr\n",
    "    self.b -= db * self.lr\n",
    "\n",
    "MyLogisticRegression._optimize = optimize\n",
    "\n",
    "## Testing ##\n",
    "test_mdl = MyLogisticRegression()\n",
    "test_mdl.fit(x_train, y_train, train=False)\n",
    "assert test_mdl._linear(np.array([1, 2])) == 0, \"the linear transformation is implemented correctly\"\n",
    "test_mdl._optimize(np.array([1, 2]), 3)\n",
    "assert np.mean(test_mdl.w) == np.mean(- np.array([1,2]) * LR), \"the weights are not updated correctly\"\n",
    "assert test_mdl.b == - 3 * LR, \"the bias is not updated correctly\"\n",
    "assert test_mdl._linear(np.array([1, 2])) == -0.8, \"the linear transformation is implemented correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now there are two parts of the neural network left to implement :\n",
    "\n",
    "* The `forward()` pass, which requires the implementation of a `_sigmoid()` method\n",
    "* The `backward()` pass, which requires the implementation of the binary cross entropy (`_bce()`) loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `sigmoid()` because it's easier :\n",
    "\n",
    "The sigmoid function is what makes our output layer serve as a probability of our value belonging to one or the other class because it squashes its input between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ S(x) = \\frac{1}{1 + e^{-x}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sigmoid(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    The sigmoid function squashes the input data between 0 and 1.\n",
    "    \"\"\"\n",
    "    z = np.exp(-x)\n",
    "    return 1 / (1 + z)\n",
    "    \n",
    "MyLogisticRegression._sigmoid = sigmoid\n",
    "\n",
    "## Testing ##\n",
    "x_sigmoid = np.arange(-10, 10, 0.1)\n",
    "plt.plot(x_sigmoid, [sigmoid(x) for x in x_sigmoid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've implemented sigmoid correctly, you should recognize matplotlib's output if you do a [quick google search](https://www.google.com/search?sca_esv=593424282&rlz=1C5CHFA_enFR1086FR1086&sxsrf=AM9HkKnhFXQw46XVx7yP5nyzZOxkebfGWw:1703424283700&q=sigmoid&tbm=isch&source=lnms&sa=X&sqi=2&ved=2ahUKEwiXptP6laiDAxVBU6QEHVYpCxIQ0pQJegQIDhAB&biw=1512&bih=738&dpr=2) for sigmoid !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can implement our `forward()` method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: np.ndarray):\n",
    "    \"\"\"\n",
    "    The forward method passes the input data through the model's transformations.\n",
    "    \"\"\"\n",
    "    y_pred = self._linear(x)\n",
    "    return np.array([self._sigmoid(val) for val in y_pred])\n",
    "\n",
    "MyLogisticRegression._forward = forward\n",
    "## Testing ##\n",
    "test_mdl = MyLogisticRegression()\n",
    "test_mdl.fit(x_train, y_train, train=False)\n",
    "test_mdl._optimize(np.array([-6, 2]), 3)\n",
    "outputs = test_mdl._forward(np.array([[1,2], [3,4]]))\n",
    "assert np.mean(np.round(outputs)) == 0.5, \"the forward method is not implemented correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculating derivatives using the Binary Cross Entropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ BCE =  - \\frac{1}{N} \\sum_{i=0}^N y_i * log(\\overline{y}_i) + (1 - y_i) * log(1 - \\overline{y}_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this seemingly complex formula is just an **elegant** way of saying :\n",
    "\n",
    "```python\n",
    "if y_i == 1:\n",
    "    return log(1 - ypred_i)\n",
    "else if y_i == 0:\n",
    "    return log(ypred_i)\n",
    "```\n",
    "\n",
    "Because we're in a binary classifcation, so our classes are either 0 or 1; it means that :\n",
    "* if we multiply something with with $ 1 - y_i $, it will be ignored when $y_i$ equals 1\\\n",
    "* and vice versa for multiplying by $ y_i $ when it equals 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This explanation is just to make the formula less intimidating\\\n",
    "> We recommend that you implement the formula as is, no need to deconstruct it into `if` statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and implement the BCE function below :\n",
    "\n",
    "* Remember how we turned $ \\frac{1}{N} \\sum_{i=0}^N $ into code for linear regression\n",
    "* To avoid multiplying by zero, add a very small value to `y_pred` each time you use it\n",
    "> tip: you can do some really neat things with numbers in python\\\n",
    "> for example, you can easily use scientific notation by writing $xe-n$ (python example below:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1e-2 == 0.01\n",
    "1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce(self, y_pred: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Binary Cross Entropy is a method of calculating the model's loss.\n",
    "    \"\"\"\n",
    "    return -np.mean(y * np.log(y_pred + 1e-9) + (1 - y) * np.log(1 - y_pred + 1e-9))\n",
    "\n",
    "MyLogisticRegression._bce = bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the backward, we'll spare you the math, but here are some resources if you wish to do it on your own and you get stuck or have questions:\n",
    "\n",
    "* https://www.python-unleashed.com/post/derivation-of-the-binary-cross-entropy-loss-gradient\n",
    "* https://math.stackexchange.com/a/3220477\n",
    "\n",
    "The partial derivative of $BCE$ with respect to $w$ is :\n",
    "\n",
    "$$ x^T * (\\overline{y} - y) $$\n",
    "\n",
    "And the partial derivative w.r.t. $b$ is :\n",
    "\n",
    "$$ \\overline{y} - y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a `backward()` method which computes the gradients using these formulas.\n",
    "\n",
    "* Remember that we are handling multiple inputs so you can retrieve the average of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    The backward method calculates the gradients for the weights and biases.\n",
    "    \"\"\"\n",
    "    y_pred = self._forward(x)\n",
    "    db = np.mean(y_pred - y)\n",
    "    dw = np.array([np.mean(grad) for grad in x.T @ (y_pred - y)])\n",
    "    return dw, db\n",
    "\n",
    "MyLogisticRegression._backward = backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't write tests for the above methods because we've actually reached the end of our neural network !\n",
    "\n",
    "We have all the building blocks to train and predict using our model !\n",
    "\n",
    "Run the code below (which you'll find is very similar to the one we used earlier with sklearn's methods) and compare your outputs to sklearn's !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = MyLogisticRegression()\n",
    "mdl.fit(x_train, y_train)\n",
    "# !!! There's a chance you might encounter a RunTimeWarning error here !!!\n",
    "# This is because your sigmoid function overflows\n",
    "# The training loop should still work, but you can look online for ways to fix this error if you want !\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=mdl.predict(x_train))\n",
    "plt.show()\n",
    "\n",
    "print(f\"{(mdl.predict(x_train) == y_train).mean()*100}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've got an accuracy of over 95%, it means you've done pretty well !\n",
    "\n",
    "All of this is cute and all, but writing all these functions by hand and computing the derivatives is kind of annoying, isn't it ?\n",
    "\n",
    "Sure, sklearn provides nice ready-for-use functions and we haven't really looked into the countless ways we can modify their functions to our liking, but surely there must be a middle ground... A tool that spares us most of the formulas and doesn't require us to compute derivatives by ourselves each time we change something in our model's architecture...\n",
    "\n",
    "We'll introduce such a tool in the final module of the day !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pool24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
