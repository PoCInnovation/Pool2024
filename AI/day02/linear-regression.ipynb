{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2024 ~\n",
    "- ## Day 2: Neural Networks from Scratch\n",
    "    - ### Module 1: Linear Regression\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the second day of your PoC AI Pool !\\\n",
    "We had to make sure everyone was up to speed on basic python (and ai-related python libs) knowledge before heading into the main topic of this Pool: **machine learning** !\n",
    "\n",
    "Yesterday, you learned some very useful skills which we'll put into practice in order to build our very first **neural network** :\n",
    "\n",
    "- python\n",
    "- numpy (to work with huge numbers and arrays)\n",
    "- matplotlib (to display graphs and visualise data)\n",
    "- pandas (to edit and analyse data)\n",
    "\n",
    "Before we delve deeper into the theory behind a neural network and the various exercises we will ask you to complete throughout this notebook, we'd like to present the overarching problem we will attempt to solve using machine learning !\n",
    "\n",
    "**The problem**\n",
    "Brad wishes to buy a house near his workplace. He's gathered a list of house prices in the area, as well as the houses' size in square feet.\\\n",
    "He'd like you to build a program which, given a house's size in square feet, outputs the lot's price.\n",
    "\n",
    "Here's a sample of data collected by Brad:\n",
    "\n",
    "|House price  |House size|\n",
    "|-------------|----------|\n",
    "|$ 100 000    |100 sq ft |\n",
    "|$ 200 000    |200 sq ft |\n",
    "|$ 300 000    |300 sq ft |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Disclaimer**:\\\n",
    "> Of course, Brad could just use simple math in order to solve his problem.\\\n",
    "> It doesn't require a neural network at all ! \\\n",
    "> We must, however, begin our neural journey humbly and work with overly simple problems for now, before delving into problems with thousands of parameters which would be better suited for ML.\n",
    "\n",
    "In a time where attention spans are at their lowest, we have decided to add a little exercise to reward you for reading more than most people do in a month nowadays :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# There's clearly a pattern in Brad's data, use simple math to solve his problem.\n",
    "# The goal of this exercise is to make sure you remember what you learned yesterday.\n",
    "# It goes without saying that you must use some of the libs from yesterday in order to solve this exercise !\n",
    "###\n",
    "\n",
    "def plot_prices(x: np.array, y: np.array):\n",
    "    \"\"\"This function should plot house the given house prices\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_data(filepath = \"./data/houses.csv\") -> Tuple[np.array, np.array]:\n",
    "    \"\"\"This function should retrieve the data from a csv file and transform it into a numpy array\"\"\"\n",
    "    pass\n",
    "\n",
    "plot_prices(*get_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "## Part 1: The theory (and some maths)\n",
    "\n",
    "\n",
    "Now that you have our end goal in mind, let's forget about real estate for a while and talk about neural networks !!!\n",
    "\n",
    "![Alt text](assets/image-5.png)\n",
    "\n",
    "It's likely you've dealt with functions before. Let's agree to represent them like the picture above.\n",
    "\n",
    "You have an input (x) which leads to an output (y).\n",
    "\n",
    "It's also likely you've heard about polynomial functions ( $ f(x) = ax + b $ ), which we'll represent like so:\n",
    "\n",
    "![Alt text](assets/image-6.png)\n",
    "\n",
    "With a weight (a) multiplying our input, as well as a bias (b) adding to it.\n",
    "\n",
    "Well what if I told you -- and you might realise yourself if you are familiar with pictures of neural networks -- that a neural network is simply a lot of functions mixed with each other ?\n",
    "\n",
    "![Alt text](assets/image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't you try it out for yourself ? A neural network is usually associated with a `forward` method in python implementations. This method will generally take an input, apply the model's transformations to it and return as output a prediction.\n",
    "\n",
    "For this problem, we will be implementing an algorithm called [**linear regression**](https://en.wikipedia.org/wiki/Linear_regression).\n",
    "\n",
    "Linear Regression is defined like so:\n",
    "\n",
    "$$ f(x) = wx + b $$\n",
    "\n",
    "With $ w $ and $ b $ being our model's **weight** and **bias** respectively. These are variables which are initialised as `0.0` but are gradually updated to fit our data throughout the **training process** using an algorithm called [**gradient descent**](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=485s).\n",
    "\n",
    "We'll delve into this process later, as it is a little bit more complicated and involves some math. So let's get the easy stuff out of the way and build this `forward` method :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1.2 ## I know I just said the weight and bias were supposed to start at 0.0, \n",
    "b = 0.6 ## but it wouldn't get us far if we tested the method by multiplying the input with 0\n",
    "\n",
    "def forward(x):\n",
    "    \"\"\"This method should take a number as input and return a prediction by applying the linear regression formula\"\"\"\n",
    "    ## apply linear regression to x and return the result\n",
    "    pass\n",
    "\n",
    "assert forward(15) == 18.6, \"The forward method is incorrectly implemented\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that wasn't too hard !\n",
    "\n",
    "But obviously, the forward function by itself is useless. The idea is for **w** and **b** to adjust **x** in such a way that it gets as close to **y** (the true value) as possible.\n",
    "\n",
    "> To make things clear, in Brad's problem, **x** would correspond to the house **size** and **y** would correspond to the house **price**.\\\n",
    "> Our model makes predictions on **y** (the house's price) given **x** (its size).\n",
    "\n",
    "In order to update **w** and **b** accordingly, we first need to check how accurate our model is.\n",
    "\n",
    "To do so, we'll use the [**mean squared error (MSE)**](https://en.wikipedia.org/wiki/Mean_squared_error) loss function to measure the difference between our model's (currently baseless) predictions and the actual data collected by Brad.\n",
    "\n",
    "The MSE is defined by:\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{i=1}^n  (y_i - \\overline{y}_i)^2 $$\n",
    "\n",
    "In other words, it is the average of the squared differences between the predictions and the truth values.\n",
    "\n",
    "The closer the result of this function is to zero, the more accurate our model becomes. So our job is to update **w** and **b** in such a way as to decrease this value, called **loss**.\n",
    "\n",
    "> **Mathematical explanation (skip this part if you're lazy; if you aren't, grab a notebook and do the equations yourself !)**: \\\n",
    "> In order to achieve this, we will use the mathematical notion of partial derivatives.\\\n",
    "> Because we want to minimize the MSE with respect to **w** and **b**, we need to obtain the partial derivatives with respect to both **w** and **b**.\\\n",
    "> First, you can replace the MSE function with a more detailed version, since we know that $ \\overline{y}_i $ can also be written as $ wx + b $ since $ \\overline{y}_i $ represents the model's prediction.\\\n",
    "> This gives us:\n",
    "> $$ MSE = \\frac{1}{N} \\sum_{i=1}^n  (y_i - (wx_i + b))^2 $$ \n",
    "> Now, let's retrieve the partial derivatives of MSE with respect to **w** and **b** (we will use these derivatives in order to update the parameters later)\n",
    "> You might be familiar with derivatives, they are used to observe the rate of change of a function with respect to a variable, by turning a function like $ f(x) = x^2 $ into $ g(x) = 2x $\n",
    "> Partial derivatives are similar, except they are used in situations where a function has multiple variables (like a neural network)\\\n",
    "> A partial derivate follows the rules of a regular derivative except every variable that isn't the focus becomes a constant:\n",
    "> $$ \\frac{d_{MSE}}{d_w} = \\frac{1}{N} \\sum_{i=1}^n  -2(y_i - (wx_i + b))^2 $$ \n",
    "> $$ \\frac{d_{MSE}}{d_b} = \\frac{1}{N} \\sum_{i=1}^n  -(y_i - (wx_i + b))^2 $$ \n",
    "> For the partial derivative of **w**, we apply the chain rule:\\\n",
    "> We now that the derivative of $ y_i - (wx_i + b) $ is $ -x $ with respect to **w**.\\\n",
    "> Using the chain rule, we can conclude that, because the derivative of $ x^2 $ is $ 2x $, the derivative of MSE with respect to **w** is $ -2(y_i - (wx_i + b))^2 $\\\n",
    "> Same applies to $ \\frac{d_{MSE}}{d_b} $, we apply the chain rule to $ -1 $ which is the derivative of $ y_i - (wx_i + b) $ with respect to **b**\\\n",
    "\n",
    "Now that we know how to minimize the MSE, we can update **w** and **b** using the following formula:\n",
    "\n",
    "$$ w_{new} = w_{old} - \\alpha  \\frac{d_{MSE}}{d_w} $$\n",
    "$$ b_{new} = b_{old} - \\alpha  \\frac{d_{MSE}}{d_b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## Part 2: The building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 0, 0\n",
    "\n",
    "def gradient_descent(x: list[float], y: list[float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    The goal of this method is, given x and y, to compute the partial derivative with respect to w and b as explained above\n",
    "    The result of the function must be a tuple containing the two derivatives\n",
    "    Don't forget to retrieve the average value\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    dl_dw = 0.0\n",
    "    dl_db = 0.0\n",
    "\n",
    "    ## Compute and return the partial derivatives\n",
    "    pass\n",
    "\n",
    "dl_dw, dl_db = gradient_descent([10, 20, 30, 40, 50], [20, 40, 60, 80, 100])\n",
    "assert (dl_dw, dl_db) == (-4400.0, -120.0), \"The answer is probably incorrect !\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we need a method to update **w** and **b** using the result of `gradient_descent` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(w: float, b: float, dl_dw: float, dl_db: float, alpha: float) -> Tuple[float, float]:\n",
    "    ## Update w and b\n",
    "    ##\n",
    "    return w, b\n",
    "\n",
    "assert optimizer(w, b, dl_dw, dl_db, alpha=5e-4) == (2.2, 0.06), \"Not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome ! We've now gathered all the ingredients to build our very own linear regression !\n",
    "\n",
    "But ingredients are not enough; we should think about the dosage as well !\n",
    "\n",
    "In the code below, we've generated some data for you using [np.linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html) to generate evenly spaced numbers between 100 and 500 (neural networks have a harder time dealing with bigger numbers, so we can [normalize](https://developers.google.com/machine-learning/data-prep/transform/normalization?hl=en) our house prices and sizes and divide them by 100).\n",
    "\n",
    "\n",
    "We're also initialising two empty lists, which we'll use to store our model's predictions as well as loss.\n",
    "\n",
    "In the **## Parameters ##** section, we initialize weights and biases to 0.0. These values will be updated with **gradient descent**.\n",
    "\n",
    "Finally, we have what are known as **hyper-parameters**. Why don't you try and choose these values ?\n",
    "\n",
    "Take a moment to look up the terms **EPOCH** and **Learning rate** on the internet and try to find a good value for them. Do the same for the amount of samples in `np.linspace`.\n",
    "\n",
    "There is no arbitrary value for these constants, it depends on what you expect from your network and what your training data is like. We will see more of these hyper parameters as we advance through this pool, but for now try and look these up !\n",
    "\n",
    "Try and understand what influence these values have by taking these statements in account :\n",
    "\n",
    "- With a low amount of training samples, the network will train faster, but it also means it will have less experience\n",
    "- With a low epoch count, the network will train faster, but it will have less time to correct itself\n",
    "- With a high learning rate, the network will make more rash decisions instead of taking its time to learn (you can see it as a sort of **confidence** rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Data #########################\n",
    "x_values = None # Choose the amount of samples of data you wish to train on\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "y_values = [100*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "x_train /= 100\n",
    "y_train /= 100\n",
    "N = len(x_train)\n",
    "###################### Logging ######################\n",
    "global_preds = []\n",
    "loss_history = []\n",
    "##################### Parameters ####################\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "################## Hyperparameters ##################\n",
    "EPOCH = None # Choose the amount of times you wish to iterate on the training data\n",
    "LR = None # Choose how quick your neural network will learn (faster != better)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "## Part 3: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have all we need to create our training loop !\n",
    "\n",
    "You will be asked to rewrite some of the methods from before and place them all together in one loop which iterates over the epochs !\n",
    "\n",
    "Don't forget: a neural network consists of :\n",
    "\n",
    "- Making a prediction\n",
    "- Comparing the prediction with the truth\n",
    "- Reviewing the model's \"loss\"\n",
    "- Updating the model's parameters\n",
    "- (optional) Logging the training data in order to draw graphs and analytics later !\n",
    "\n",
    "Good luck !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPOCH * 100):\n",
    "    dl_dw = 0.0\n",
    "    dl_db = 0.0\n",
    "\n",
    "    # Logging predictions\n",
    "    epoch_preds = []\n",
    "    for i in range(N):\n",
    "        epoch_preds.append(forward(x_train[i]))\n",
    "    if e % 25 == 0:\n",
    "        ratio = np.all(epoch_preds / y_train >= 0.95) and np.all((epoch_preds / y_train) <= 1.05)\n",
    "        if ratio == True:\n",
    "            break\n",
    "    if e % 50 == 0:\n",
    "        global_preds.append(np.array(epoch_preds))\n",
    "\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        # Prediction\n",
    "        pred = None\n",
    "\n",
    "        # Gradient descent\n",
    "        dl_dw += None\n",
    "        dl_db += None\n",
    "\n",
    "    # Getting the average values\n",
    "    dl_dw *= None\n",
    "    dl_db *= None\n",
    "\n",
    "    # Optimization\n",
    "    w = None\n",
    "    b = None\n",
    "\n",
    "    # Logging loss\n",
    "    total_error = 0.0\n",
    "    for i in range(N):\n",
    "        total_error += None\n",
    "    loss_history.append(total_error / N)\n",
    "\n",
    "\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train, global_preds[0], label=\"predictions before training\")\n",
    "plt.plot(x_train, global_preds[1], label=\"predictions after 50 epochs\")\n",
    "plt.plot(x_train, global_preds[-1], label=f\"predictions after {EPOCH} epochs\")\n",
    "plt.plot(x_train, y_train, '--', label=\"real prices\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats ! If you're reading this and your model has a high accuracy (represented by the green and red lines being very close to each other on the above graph), it means you've managed to build and train your very own linear regression.\n",
    "\n",
    "This algorithm is great for training models to forecast linear datasets.\n",
    "\n",
    "There is another major application of machine learning, **Logistic Regression**, which is used to make classification predictions (i.e: predicting whether an image of a cat or dog is shown).\n",
    "\n",
    "In the next section, we'll see how that is done !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's not forget about Brad ! \n",
    "\n",
    "Here's a program that gives him an estimation of the house's price given a size using your model's parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_house_price(size: int):\n",
    "    size /= 100\n",
    "    pred = forward(size)\n",
    "    return pred * 100\n",
    "\n",
    "answer = predict_house_price(200)\n",
    "assert answer >= 19000 and answer <= 21000\n",
    "print(f\"The house will cost ${answer:.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pool24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
